# üîß Next Steps: Real OCR Integration

## ‚ùå –¢–µ–∫—É—â–∞—è –ø—Ä–æ–±–ª–µ–º–∞

**Tesseract.js –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ Vercel serverless:**
```
Error: Cannot find module '/var/task/.next/server/app/worker-script/node/index.js'
```

**–ü—Ä–∏—á–∏–Ω–∞:**
- Tesseract.js –∏—Å–ø–æ–ª—å–∑—É–µ—Ç Node.js Worker Threads
- Vercel serverless functions –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç worker threads
- –≠—Ç–æ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ AWS Lambda (–Ω–∞ –∫–æ—Ç–æ—Ä–æ–º —Ä–∞–±–æ—Ç–∞–µ—Ç Vercel)

**–í—Ä–µ–º–µ–Ω–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ:**
- OCR –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç stub –¥–∞–Ω–Ω—ã–µ (—Ñ–µ–π–∫–æ–≤—ã–µ)
- –û—Å—Ç–∞–ª—å–Ω–æ–π flow —Ä–∞–±–æ—Ç–∞–µ—Ç (LLM, generation, UI)
- –ú–æ–∂–Ω–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –≤—Å—ë –∫—Ä–æ–º–µ —Ä–µ–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞

---

## ‚úÖ –õ—É—á—à–µ–µ —Ä–µ—à–µ–Ω–∏–µ: Google Cloud Vision API

### –ü–æ—á–µ–º—É Google Vision:
- ‚úÖ **–†–∞–±–æ—Ç–∞–µ—Ç –≤ serverless** (HTTP API, –Ω–µ —Ç—Ä–µ–±—É–µ—Ç workers)
- ‚úÖ **–û—á–µ–Ω—å –±—ã—Å—Ç—Ä–æ** (1-2 —Å–µ–∫—É–Ω–¥—ã –≤–º–µ—Å—Ç–æ 10-15)
- ‚úÖ **–õ—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å** (Google's ML –º–æ–¥–µ–ª–∏)
- ‚úÖ **–ü–æ–¥–¥–µ—Ä–∂–∫–∞ 50+ —è–∑—ã–∫–æ–≤** –≤–∫–ª—é—á–∞—è Indonesian
- ‚úÖ **Reasonable —Ü–µ–Ω–∞** ($1.50 –∑–∞ 1000 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π)

### –ö–∞–∫ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å:

#### 1. –°–æ–∑–¥–∞—Ç—å Google Cloud Project

```bash
# 1. –ü–µ—Ä–µ–π—Ç–∏ –Ω–∞ https://console.cloud.google.com
# 2. Create New Project
# 3. Enable Vision API
# 4. Create Service Account
# 5. Download JSON key
```

#### 2. –î–æ–±–∞–≤–∏—Ç—å credentials –≤ Vercel

```bash
# Vercel Dashboard ‚Üí Settings ‚Üí Environment Variables

# Add:
GOOGLE_VISION_CREDENTIALS=<paste full JSON key here as string>
# –∏–ª–∏
GOOGLE_APPLICATION_CREDENTIALS=/tmp/vision-key.json
```

#### 3. –û–±–Ω–æ–≤–∏—Ç—å package.json

```json
{
  "dependencies": {
    "@google-cloud/vision": "^4.0.0"
  }
}
```

#### 4. –û–±–Ω–æ–≤–∏—Ç—å lib/ocr.ts

```typescript
import vision from '@google-cloud/vision';
import type { OCRResult, TextBlock } from '@/types/creative';

let visionClient: vision.ImageAnnotatorClient | null = null;

function getVisionClient() {
  if (!visionClient) {
    // Parse credentials from env var
    const credentials = JSON.parse(
      process.env.GOOGLE_VISION_CREDENTIALS || '{}'
    );
    
    visionClient = new vision.ImageAnnotatorClient({
      credentials,
    });
  }
  return visionClient;
}

export async function runOCR(imageBuffer: Buffer): Promise<OCRResult> {
  try {
    console.log('üîç Starting OCR with Google Vision API...');
    
    const client = getVisionClient();
    
    // Call Vision API
    const [result] = await client.textDetection({
      image: { content: imageBuffer },
    });
    
    const detections = result.textAnnotations || [];
    
    if (detections.length === 0) {
      console.warn('‚ö†Ô∏è No text detected');
      return getStubOCRResult();
    }
    
    // First annotation is full text
    const fullText = detections[0]?.description || '';
    
    // Rest are individual words/blocks
    const blocks: TextBlock[] = detections.slice(1).map(annotation => {
      const vertices = annotation.boundingPoly?.vertices || [];
      const x = Math.min(...vertices.map(v => v.x || 0));
      const y = Math.min(...vertices.map(v => v.y || 0));
      const maxX = Math.max(...vertices.map(v => v.x || 0));
      const maxY = Math.max(...vertices.map(v => v.y || 0));
      
      return {
        text: annotation.description || '',
        bbox: {
          x,
          y,
          width: maxX - x,
          height: maxY - y,
        },
        confidence: annotation.confidence || 0.9,
      };
    });
    
    // Group blocks on same line
    const groupedBlocks = groupTextBlocks(blocks);
    
    console.log(`‚úÖ Vision API: ${groupedBlocks.length} blocks detected`);
    
    return {
      blocks: groupedBlocks,
      fullText,
      confidence: 0.95, // Vision API is very accurate
      language: detectLanguage(fullText),
    };
  } catch (error) {
    console.error('‚ùå Vision API error:', error);
    return getStubOCRResult();
  }
}
```

#### 5. Deploy

```bash
git add -A
git commit -m "‚ú® Integrate Google Cloud Vision API for OCR"
git push origin main

# Vercel will auto-deploy
```

---

## üîÑ –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è

### Option 2: OCR.space API (Free tier)

**Pros:**
- Free tier: 25,000 requests/month
- HTTP API (—Ä–∞–±–æ—Ç–∞–µ—Ç –≤ serverless)
- –ü—Ä–æ—Å—Ç–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è

**Cons:**
- –ú–µ–¥–ª–µ–Ω–Ω–µ–µ —á–µ–º Google Vision
- –ú–µ–Ω–µ–µ —Ç–æ—á–Ω—ã–π
- Rate limits

```typescript
// lib/ocr.ts
export async function runOCR(imageBuffer: Buffer): Promise<OCRResult> {
  const base64Image = imageBuffer.toString('base64');
  
  const response = await fetch('https://api.ocr.space/parse/image', {
    method: 'POST',
    headers: {
      'apikey': process.env.OCR_SPACE_API_KEY!,
    },
    body: JSON.stringify({
      base64Image: `data:image/png;base64,${base64Image}`,
      language: 'eng',
    }),
  });
  
  const data = await response.json();
  const fullText = data.ParsedResults[0]?.ParsedText || '';
  
  // Convert to our format...
}
```

**Free API Key:** https://ocr.space/ocrapi

### Option 3: AWS Textract

**Pros:**
- –û—á–µ–Ω—å —Ç–æ—á–Ω—ã–π
- –•–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
- Managed by AWS

**Cons:**
- –î–æ—Ä–æ–∂–µ ($1.50 –∑–∞ 1000 + AWS costs)
- –°–ª–æ–∂–Ω–µ–µ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å
- Overkill –¥–ª—è –∫—Ä–µ–∞—Ç–∏–≤–æ–≤

### Option 4: Azure Computer Vision

**Pros:**
- –¢–æ—á–Ω—ã–π
- –ë—ã—Å—Ç—Ä—ã–π
- Managed

**Cons:**
- –ù—É–∂–µ–Ω Azure account
- Pricing complex

---

## üí∞ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ü–µ–Ω (–¥–ª—è 161 –∫—Ä–µ–∞—Ç–∏–≤–∞)

| Service | Cost per image | Total for 161 | Speed |
|---------|----------------|---------------|-------|
| **Google Vision** | $0.0015 | **$0.24** | 1-2s ‚ö° |
| **OCR.space Free** | $0 (up to 25k/mo) | **FREE** | 3-5s |
| **AWS Textract** | $0.0015 + AWS | $0.30+ | 2-3s |
| **Tesseract.js** | $0 | ‚ùå Not working | 10-15s |

---

## üéØ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è

### –î–ª—è MVP / Testing:
**–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ OCR.space (Free tier)**
- –ë–µ—Å–ø–ª–∞—Ç–Ω–æ –¥–æ 25k requests/month
- –ü—Ä–æ—Å—Ç–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è (5 –º–∏–Ω—É—Ç)
- –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–æ—á–Ω—ã–π –¥–ª—è –∫—Ä–µ–∞—Ç–∏–≤–æ–≤

### –î–ª—è Production:
**–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ Google Vision API**
- –õ—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å
- –°–∞–º—ã–π –±—ã—Å—Ç—Ä—ã–π
- –ù–∞–¥—ë–∂–Ω—ã–π
- –î–µ—à—ë–≤—ã–π ($0.24 –∑–∞ 161 –∫—Ä–µ–∞—Ç–∏–≤)

---

## üìù Quick Start: OCR.space (5 –º–∏–Ω—É—Ç)

### 1. –ü–æ–ª—É—á–∏—Ç—å API key
```
https://ocr.space/ocrapi/freekey
```

### 2. –î–æ–±–∞–≤–∏—Ç—å –≤ Vercel
```bash
# Vercel ‚Üí Settings ‚Üí Environment Variables
OCR_SPACE_API_KEY=<your-key>
```

### 3. –û–±–Ω–æ–≤–∏—Ç—å lib/ocr.ts

```typescript
export async function runOCR(imageBuffer: Buffer): Promise<OCRResult> {
  try {
    console.log('üîç Starting OCR with OCR.space...');
    
    const formData = new FormData();
    formData.append('base64Image', 
      `data:image/png;base64,${imageBuffer.toString('base64')}`
    );
    formData.append('language', 'eng');
    formData.append('isOverlayRequired', 'true'); // Get bounding boxes
    
    const response = await fetch('https://api.ocr.space/parse/image', {
      method: 'POST',
      headers: {
        'apikey': process.env.OCR_SPACE_API_KEY!,
      },
      body: formData,
    });
    
    const data = await response.json();
    
    if (!data.ParsedResults?.[0]) {
      throw new Error('OCR failed');
    }
    
    const result = data.ParsedResults[0];
    const fullText = result.ParsedText || '';
    
    // Parse words with bounding boxes
    const blocks: TextBlock[] = (result.TextOverlay?.Lines || []).flatMap(
      (line: any) => line.Words.map((word: any) => ({
        text: word.WordText,
        bbox: {
          x: word.Left,
          y: word.Top,
          width: word.Width,
          height: word.Height,
        },
        confidence: 0.9, // OCR.space doesn't provide per-word confidence
      }))
    );
    
    return {
      blocks: groupTextBlocks(blocks),
      fullText,
      confidence: 0.85,
      language: detectLanguage(fullText),
    };
  } catch (error) {
    console.error('‚ùå OCR.space error:', error);
    return getStubOCRResult();
  }
}
```

### 4. Install FormData polyfill

```bash
npm install form-data
```

```typescript
// Add to lib/ocr.ts
import FormData from 'form-data';
```

### 5. Deploy

```bash
git commit -am "‚ú® Integrate OCR.space API"
git push origin main
```

---

## üöÄ –¢–µ–∫—É—â–∏–π —Å—Ç–∞—Ç—É—Å

### ‚úÖ –ß—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç:
- UI —Å –º–æ–¥–∞–ª–∞–º–∏
- –ê–≤—Ç–æ–∞–Ω–∞–ª–∏–∑ –ø–µ—Ä–≤—ã—Ö 6
- –ö–Ω–æ–ø–∫–∏ –Ω–∞ –∫–∞—Ä—Ç–æ—á–∫–∞—Ö
- LLM analysis (roles)
- –ì–µ–Ω–µ—Ä–∞—Ü–∏—è (3 —Ä–µ–∂–∏–º–∞)
- –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ Supabase

### ‚ö†Ô∏è –ß—Ç–æ –ù–ï —Ä–∞–±–æ—Ç–∞–µ—Ç:
- **–†–µ–∞–ª—å–Ω—ã–π OCR** (–≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç stub –¥–∞–Ω–Ω—ã–µ)

### üîß –ß—Ç–æ –Ω—É–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å:
1. **–í—ã–±—Ä–∞—Ç—å OCR —Å–µ—Ä–≤–∏—Å** (—Ä–µ–∫–æ–º–µ–Ω–¥—É—é OCR.space –¥–ª—è –Ω–∞—á–∞–ª–∞)
2. **–ü–æ–ª—É—á–∏—Ç—å API key**
3. **–û–±–Ω–æ–≤–∏—Ç—å `lib/ocr.ts`** (–∫–æ–¥ –≤—ã—à–µ)
4. **Deploy**
5. **–ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –∫—Ä–µ–∞—Ç–∏–≤–∞—Ö**

---

## ‚è±Ô∏è –í—Ä–µ–º—è –Ω–∞ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é

- **OCR.space:** 5-10 –º–∏–Ω—É—Ç
- **Google Vision:** 20-30 –º–∏–Ω—É—Ç (–Ω—É–∂–µ–Ω Google Cloud account)
- **AWS Textract:** 30-60 –º–∏–Ω—É—Ç (—Å–ª–æ–∂–Ω–µ–µ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å)

---

**–î–∞–π—Ç–µ –∑–Ω–∞—Ç—å –∫–∞–∫–æ–π —Å–µ—Ä–≤–∏—Å —Ö–æ—Ç–∏—Ç–µ - –ø–æ–º–æ–≥—É –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å! üöÄ**

–†–µ–∫–æ–º–µ–Ω–¥—É—é –Ω–∞—á–∞—Ç—å —Å **OCR.space** (–±–µ—Å–ø–ª–∞—Ç–Ω–æ + –±—ã—Å—Ç—Ä–æ).

